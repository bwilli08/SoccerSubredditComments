{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval from Google BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through [this reddit post](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/), I found the Reddit comments dataset on [Google BigQuery](https://cloud.google.com/bigquery/). As a student, I was able to create a free, one-year account that has $300 worth of compute credits. A preview of this dataset is shown below:\n",
    "\n",
    "![Preview of Comments Dataset](RedditComments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I decided to find the subreddits associated with the clubs in the top four tiers of English soccer. I was able to find a majority of these by looking at [soccer-related subreddits](https://www.reddit.com/r/soccer/wiki/relatedsubreddits) and cross-referencing it with a list of teams in each league. I compiled these into a .csv file and uploaded them to BigQuery as a standalone table.\n",
    "\n",
    "![My Subreddit Table](Subreddits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I had a list of subreddits I cared about, I could query the Reddit Comments table and filter out any comments from subreddits that weren't in my list. As a result of some data exploration I did in the BigQuery console, I also filtered out comments that were deleted, and ended up only retrieving comments from Premier League subreddits. This last choice was after realizing that the non-Premier League subreddits are very small (100-500 total users) and relatively inactive (only a few daily active users), making textual analysis almost worthless for those subreddits.\n",
    "\n",
    "Also, I decided to retrieve only specific attributes of the Comment dataset:\n",
    "* body\n",
    "* subreddit\n",
    "* author\n",
    "* score\n",
    "* ups\n",
    "* downs\n",
    "* created_utc\n",
    "\n",
    "All of the other fields are either rarely populated or irrelevant to the analysis I'm doing.\n",
    "\n",
    "![Comment Query](CommentQuery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this query is too large for Google BigQuery to return on the console, so I had to export the results to a Google Cloud Storage bucket. By deciding the prefix of each file, the later reading of the data was fairly simple. Once in the storage bucket, I downloaded each .csv file locally and began the import process.\n",
    "\n",
    "![Google Cloud Storage bucket](GoogleDataStorage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Comment Data\n",
    "\n",
    "This is necessary because my computer can't hold the entire 2.6 GB dataset (plus sentiment analysis) in memory all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_using_python(unicode_string):\n",
    "    return (unicode_string.encode('ascii', 'ignore')).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def convertToDate(timestamp):\n",
    "    return datetime.datetime.fromtimestamp(int(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def transformText(text):\n",
    "    return re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', html.unescape(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't create the subreddit index yet! It slows the insertion process by an extreme amount.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import sql\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "\n",
    "engine = create_engine('mysql://wilbren:Aug9th95@127.0.0.1/data301')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    engine.execute(\n",
    "        \"CREATE TABLE IF NOT EXISTS comments ( \\\n",
    "        id INT PRIMARY KEY AUTO_INCREMENT, \\\n",
    "        body TEXT, \\\n",
    "        author VARCHAR(20), \\\n",
    "        subreddit VARCHAR(20), \\\n",
    "        created_utc DATETIME, \\\n",
    "        score INT \\\n",
    "        )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "def read_and_insert(filename):\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, dtype='unicode')\n",
    "    df = df.drop(['ups', 'downs'], axis=1)\n",
    "    print(\"Original Length: %d\" % len(df))\n",
    "\n",
    "    df = df.dropna(how='any', subset=['body'])\n",
    "    df['body'] = df['body'].apply(transformText)\n",
    "    print(\"After dropping null bodys: %d\" % len(df))\n",
    "\n",
    "    df = df[df['body'] != '[removed]' & df['author'] != '[deleted]']\n",
    "    print(\"After dropping [removed] bodys and [deleted] authors: %d\" % len(df))\n",
    "    \n",
    "    df = df.applymap(lambda x: filter_using_python(x) if type(x) is str else x)\n",
    "    df['created_utc'] = df['created_utc'].apply(convertToDate)\n",
    "    df = df[df['created_utc'] > '2017-06-01']\n",
    "    print(\"Dropping comments before this season: %d\" % len(df))\n",
    "    \n",
    "    df['score'] = df['score'].astype(int)\n",
    "\n",
    "    print(\"Inserting into database...\")\n",
    "    df.to_sql('comments', con=engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_prefix = \"/mnt/c/Users/Brent Williams/Downloads/\"\n",
    "# \n",
    "# for x in range(0, 7, 1):\n",
    "#     if x not in finished_files:\n",
    "#         filename = file_prefix + \"comments\" + str(x).zfill(12)\n",
    "#         read_and_insert(filename)\n",
    "#         finished_files.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.execute(\"CREATE INDEX subreddit ON comments (subreddit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sentiment Analysis Scores\n",
    "\n",
    "**Again, don't create the FK constraint until after table creation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sqlalchemy/engine/default.py:507: Warning: (1050, \"Table 'comments_meta' already exists\")\n",
      "  cursor.execute(statement, parameters)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f365efb1ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    engine.execute(\n",
    "        \"CREATE TABLE IF NOT EXISTS comments_meta (\\\n",
    "        id INT PRIMARY KEY, \\\n",
    "        positivity DECIMAL(5, 3), \\\n",
    "        negativity DECIMAL(5, 3), \\\n",
    "        net_score DECIMAL(5, 3) \\\n",
    "        )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentimentAnalysis(comment):\n",
    "    return sid.polarity_scores(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM comments c LEFT JOIN comments_meta cm USING (id) WHERE cm.id is null LIMIT 140000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** If you don't already have the sentiment analysis data locally, the following cell takes a long time to run (approximately an hour) since it has to retrieve every comment, give it a positive/negative score, and then insert those scores into the comments_meta table. We only do this for 140,000 comments at a time, so that this cell can be interrupted and re-run without much loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "comments = pd.read_sql(query, engine)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    while len(comments) > 0:\n",
    "        print(\".\", end='')\n",
    "\n",
    "        scores = comments['body'].apply(sentimentAnalysis)\n",
    "        comments['positivity'] = scores.apply(lambda x: x['pos'])\n",
    "        comments['negativity'] = scores.apply(lambda x: x['neg'])\n",
    "        comments['net_score'] = comments['positivity'] - comments['negativity']\n",
    "\n",
    "        comments_meta = comments[['id', 'positivity', 'negativity', 'net_score']]\n",
    "        comments_meta.to_sql('comments_meta', con=engine, index=False, if_exists='append')\n",
    "\n",
    "        comments = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.execute(\"ALTER TABLE comments_meta ADD CONSTRAINT fk_comment_id FOREIGN KEY (id) REFERENCES comments(id) ON DELETE CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
